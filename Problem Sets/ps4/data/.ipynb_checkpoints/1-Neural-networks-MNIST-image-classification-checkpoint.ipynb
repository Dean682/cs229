{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "    return x, y\n",
    "\n",
    "def nn_test(data, labels, params):\n",
    "    h, output, cost = forward_prop(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    accuracy = (np.argmax(output,axis=1) == np.argmax(labels,axis=1)).sum() * 1. / labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def prepare_data(random_seed=100):\n",
    "    np.random.seed(random_seed) # for reproducibility\n",
    "    trainData, trainLabels = readData('mnist/images_train.csv', 'mnist/labels_train.csv')\n",
    "    trainLabels = one_hot_labels(trainLabels)\n",
    "    p = np.random.permutation(60000)\n",
    "    trainData = trainData[p,:]\n",
    "    trainLabels = trainLabels[p,:]\n",
    "\n",
    "    devData = trainData[0:10000,:]\n",
    "    devLabels = trainLabels[0:10000,:]\n",
    "    trainData = trainData[10000:,:]\n",
    "    trainLabels = trainLabels[10000:,:]\n",
    "\n",
    "    mean = np.mean(trainData)\n",
    "    std = np.std(trainData)\n",
    "    trainData = (trainData - mean) / std\n",
    "    devData = (devData - mean) / std\n",
    "\n",
    "    testData, testLabels = readData('mnist/images_test.csv', 'mnist/labels_test.csv')\n",
    "    testLabels = one_hot_labels(testLabels)\n",
    "    testData = (testData - mean) / std\n",
    "\n",
    "    return trainData, trainLabels, devData, devLabels, testData, testLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement components of NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathrm{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}} = \\frac{e^{x_i + c}}{\\sum_j e^{x_j + c}} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since adding a constant won't change softmax results, so we remove the maximum of $x$ for numerical stability. Reference of previous work from cs231n: https://github.com/zyxue/stanford-cs231n/blob/master/assignment1/cs231n/classifiers/softmax.py#L65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for input. \n",
    "    Use tricks from previous assignment to avoid overflow\n",
    "    \n",
    "    x is of shape: B x K\n",
    "    B: batch size\n",
    "    K: number of classes\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    c = np.max(x, axis=1, keepdims=True)\n",
    "    numerator = np.exp(x - c)\n",
    "    denominator = np.sum(numerator, axis=1, keepdims=True)\n",
    "    s = numerator / denominator\n",
    "    ### END YOUR CODE\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathrm{sigmoid}(x)_i &= \\frac{1}{1 + e^{-x}} = \\frac{e^{x}}{e^{x} + 1}\n",
    "\\end{align*}\n",
    "\n",
    "For numerical stability, use the 1st equation for positive x, and the 2nd equation for negative x. Reference: https://github.com/zyxue/stanford-cs231n/blob/5727c79970285e19c005b19237ade7412f676a40/assignment3/cs231n/rnn_layers.py#L264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    \n",
    "    x is of shape: B x H\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    pos_mask = (x >= 0)\n",
    "    neg_mask = (x < 0)\n",
    "\n",
    "    # specify dtype! otherwise, it may all becomes zero, this could have different\n",
    "    # behaviors depending on numpy version\n",
    "    z = np.zeros_like(x, dtype=float)\n",
    "    z[pos_mask] = np.exp(-x[pos_mask])\n",
    "    z[neg_mask] = np.exp(x[neg_mask])\n",
    "\n",
    "    top = np.ones_like(x, dtype=float)\n",
    "    top[neg_mask] = z[neg_mask]\n",
    "    s = top / (1 + z)\n",
    "    ### END YOUR CODE\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation equations:\n",
    "\n",
    "\\begin{align*}\n",
    "h &= sigmoid(z_1) = s(z_1) = s(x W_1 + b_1) \\\\\n",
    "\\hat y &= softmax(z_2) = f(z_2) = f(hW_2 + b_2) \\\\\n",
    "J &= CE(y, \\hat y) = - \\sum_{k=1}^{K} y_k \\log{\\hat y_k}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    return hidder layer, output(softmax) layer and loss\n",
    "    \n",
    "    data is of shape: B x D\n",
    "    labels is of shape: B x K\n",
    "    \"\"\"\n",
    "    W1 = params['W1'] # D x H\n",
    "    b1 = params['b1'] # B x H\n",
    "    W2 = params['W2'] # H x K\n",
    "    b2 = params['b2'] # B x K\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    z1 = data.dot(W1) + b1\n",
    "    h = sigmoid(z1)\n",
    "    z2 = h.dot(W2) + b2\n",
    "    y = softmax(z2)\n",
    "\n",
    "    loss = - np.multiply(labels, np.log(y + 1e-16)).sum()\n",
    "    loss /= data.shape[0]\n",
    "    ### END YOUR CODE\n",
    "    return h, y, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation equations, I derived and implemented these before from another NN course: https://github.com/zyxue/stanford-cs224n/blob/master/assignment1/q2-Neural-network-basics.ipynb.\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_1 &= \\frac{\\partial J}{\\partial z_2} = \\hat y - y \\\\\n",
    "\\frac{\\partial J}{\\partial W_2}  &= \\delta_1 \\frac{\\partial z_2}{\\partial W_2} = h^T \\delta_1 \\\\\n",
    "\\frac{\\partial J}{\\partial b_2}  &=\\delta_1 \\frac{\\partial z_2}{\\partial b_2} = \\delta_1 \\\\\n",
    "\\frac{\\partial J}{\\partial h}  &= \\delta_1 \\frac{\\partial z_2}{\\partial h} = \\delta_1 W_2^T\\\\\n",
    "\\delta_2 &= \\frac{\\partial J}{\\partial z_1}  = \\frac{\\partial J}{\\partial h} \\sigma'(z_1) =   \\delta_1 W_2^T \\circ h(1 - h) \\\\\n",
    "\\frac{\\partial J}{\\partial W_1} &= \\delta_2 \\frac{\\partial z_1}{\\partial W_1} = x^T \\delta_2 \\\\\n",
    "\\frac{\\partial J}{\\partial b_1} &= \\delta_2 \\frac{\\partial z_1}{\\partial b_1} = \\delta_2 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    return gradient of parameters\n",
    "    \"\"\"\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # The gradient calculation has been checked with gradient_check, and should be correct\n",
    "    h, y, cost = forward_prop(data, labels, params)\n",
    "\n",
    "    delta_1 = (y - labels) \n",
    "    gradW2 = np.dot(h.T, delta_1)\n",
    "    gradb2 = np.sum(delta_1, axis=0, keepdims=True)\n",
    "    \n",
    "    # Note: in the above\n",
    "    # matrix multiple sums the grad over all samples in the batch\n",
    "    # but for b: it needs sum up mannually\n",
    "    \n",
    "    delta_2 = np.multiply(np.dot(delta_1, W2.T), h * (1 - h))\n",
    "    gradW1 = np.dot(data.T, delta_2)\n",
    "    gradb1 = np.sum(delta_2, axis=0, keepdims=True)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    grad = {}\n",
    "    # important: normalize by batch size!\n",
    "    B = data.shape[0]\n",
    "    grad['W1'] = gradW1 / B\n",
    "    grad['W2'] = gradW2 / B\n",
    "    grad['b1'] = gradb1 / B\n",
    "    grad['b2'] = gradb2 / B\n",
    "\n",
    "    lamb = params['lambda']\n",
    "    if lamb > 0:\n",
    "        grad['W2'] += lamb * W2 \n",
    "        grad['W1'] += lamb * W1\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(y, labels):\n",
    "    \"\"\"my own version of calculate accurancy. Acutally, there is a version of\n",
    "    `compute_accuracy` in the nn_starter.py code already\"\"\"\n",
    "    pred = np.zeros_like(y)\n",
    "    # this is wrong! keep here for \"good memory\"\n",
    "    # pred[:, np.argmax(y, axis=1)] = 1\n",
    "    # learned from https://stackoverflow.com/questions/20295046/numpy-change-max-in-each-row-to-1-all-other-numbers-to-0\n",
    "    pred[np.arange(y.shape[0]), np.argmax(y, axis=1)] = 1\n",
    "    \n",
    "    res = np.abs((pred - labels)).sum(axis=1)\n",
    "    acc = res[res == 0].shape[0] / res.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grad, learning_rate):\n",
    "    # It's minus the grad instead of plus!!!\n",
    "    params['W1'] -= learning_rate * grad['W1']\n",
    "    params['W2'] -= learning_rate * grad['W2']\n",
    "    params['b1'] -= learning_rate * grad['b1']\n",
    "    params['b2'] -= learning_rate * grad['b2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(trainData, trainLabels, devData, devLabels, \n",
    "             num_hidden=300, learning_rate=5, batch_size=1000, num_epochs=30,\n",
    "             reg_strength=0):\n",
    "    (m, n) = trainData.shape\n",
    "    params = {}\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # rename variables to more intuitive ones\n",
    "    N = m\n",
    "    D = n\n",
    "    K = trainLabels.shape[1]\n",
    "    H = num_hidden\n",
    "    B = batch_size\n",
    "    \n",
    "    params['W1'] = np.random.standard_normal((n, H))\n",
    "    params['b1'] = np.zeros((1, H), dtype=float)\n",
    "    params['W2'] = np.random.standard_normal((num_hidden, K))\n",
    "    params['b2'] = np.zeros((1, K), dtype=float)\n",
    "    params['lambda'] = reg_strength\n",
    "    \n",
    "    num_iter = int(N / B) # number of iterations per epoch\n",
    "    tr_loss, tr_metric, dev_loss, dev_metric = [], [], [], []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(i, end=',')\n",
    "        for j in range(num_iter):\n",
    "            batch_data = trainData[j * B: (j + 1) * B]\n",
    "            batch_labels = trainLabels[j * B: (j + 1) * B]\n",
    "            grad = backward_prop(batch_data, batch_labels, params)\n",
    "            update_params(params, grad, learning_rate)\n",
    "       \n",
    "        _, _y, _cost = forward_prop(trainData, trainLabels, params)\n",
    "        tr_loss.append(_cost)\n",
    "        tr_metric.append(calc_accuracy(_y, trainLabels))\n",
    "        _, _y, _cost = forward_prop(devData, devLabels, params)\n",
    "        dev_loss.append(_cost)\n",
    "        dev_metric.append(calc_accuracy(_y, devLabels))\n",
    "    ### END YOUR CODE\n",
    "    return params, tr_loss, tr_metric, dev_loss, dev_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 s, sys: 1.85 s, total: 27.2 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%time trainData, trainLabels, devData, devLabels, testData, testLabels = prepare_data(random_seed=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) Train without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "params, tr_loss, tr_metric, dev_loss, dev_metric = nn_train(\n",
    "    trainData, trainLabels, devData, devLabels, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('params_no_regularization.pkl', 'wb') as opf:\n",
    "#     pickle.dump(params, opf)\n",
    "\n",
    "# with open('params_no_regularization.pkl', 'rb') as inf:\n",
    "#     params = pickle.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, dev_metric, label='dev acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(num_epochs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, sharey=False, figsize=(12, 4))\n",
    "ax0, ax1 = axes.ravel()\n",
    "\n",
    "ax0.plot(xs, tr_loss, label='train loss')\n",
    "ax0.plot(xs, dev_loss, label='dev loss')\n",
    "ax0.legend()\n",
    "ax0.set_xlabel('# epoch')\n",
    "ax0.set_ylabel('CE loss')\n",
    "\n",
    "ax1.plot(xs, tr_metric, label='train acc')\n",
    "ax1.plot(xs, dev_metric, label='dev acc')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('# epoch')\n",
    "ax1.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Train with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "# tried multiple reg_strength values to pick a relatively good one\n",
    "params_reg, tr_loss, tr_metric, dev_loss, dev_metric = nn_train(\n",
    "    trainData, trainLabels, devData, devLabels, num_epochs=num_epochs, reg_strength=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('params_with_regularization.pkl', 'wb') as opf:\n",
    "    pickle.dump(params_reg, opf)\n",
    "    \n",
    "# # with open('params_with_regularization.pkl', 'rb') as inf:\n",
    "# #     params_reg = pickle.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(num_epochs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, sharey=False, figsize=(12, 4))\n",
    "ax0, ax1 = axes.ravel()\n",
    "\n",
    "ax0.plot(xs, tr_loss, label='train loss')\n",
    "ax0.plot(xs, dev_loss, label='dev loss')\n",
    "ax0.legend()\n",
    "ax0.set_xlabel('# epoch')\n",
    "ax0.set_ylabel('CE loss')\n",
    "\n",
    "ax1.plot(xs, tr_metric, label='train acc')\n",
    "ax1.plot(xs, dev_metric, label='dev acc')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('# epoch')\n",
    "ax1.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: without regularization, the overfitting is apparent as there is big difference between the performances on the the training and validation data in terms of both CE loss and accuracy. In contrast, with regularization, the performance on validation data follows that on training data closely, does much better than without regularization. You will see different kinds of spikes by changing the random see when calling `prepare_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) Evaluate performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = nn_test(testData, testLabels, params)\n",
    "print('Test accuracy (without regularization): {0}'.format(accuracy))\n",
    "\n",
    "accuracy = nn_test(testData, testLabels, params_reg)\n",
    "print('Test accuracy (with regularization): {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup code\n",
    "\n",
    "Below are left here for future reference, applied gradcheck during development to ensure backprop is implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradcheck_naive(f, x):\n",
    "    \"\"\"Gradient check for a function f.\n",
    "\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         cost and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "\n",
    "    confirmed it's a bit different from eval_numerical_gradient on\n",
    "    http://cs231n.github.io/optimization-1/\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x)             # Evaluate function value at original point\n",
    "    h = 1e-4                    # Do not change this!\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        # Try modifying x[ix] with h defined above to compute\n",
    "        # numerical gradients. Make sure you call random.setstate(rndstate)\n",
    "        # before calling f(x) each time. This will make it possible\n",
    "        # to test cost functions with built in randomness later.\n",
    "        old_x = x[ix]\n",
    "        x[ix] = old_x + h       # increment by h\n",
    "        random.setstate(rndstate)\n",
    "        fxph, _ = f(x)             # evalute f(x + h)\n",
    "        x[ix] = old_x - h\n",
    "        random.setstate(rndstate)\n",
    "        fxmh, _ = f(x)             # evaluate f(x - h)\n",
    "        numgrad = (fxph - fxmh) / (2 * h)  # the slope\n",
    "        x[ix] = old_x\n",
    "\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        # print('numerical gradient: {0}; reldiff: {1}'.format(numgrad, reldiff))\n",
    "        if reldiff > 1e-5:\n",
    "            print(\"Gradient check failed.\")\n",
    "            print(\"First gradient error found at index {0}\".format(str(ix)))\n",
    "            print(\"Your gradient: {0} \\t Numerical gradient: {1}\".format(\n",
    "                grad[ix], numgrad))\n",
    "            return\n",
    "        it.iternext()           # Step to next dimension\n",
    "\n",
    "    print(\"Gradient check passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_grad_check(data, labels, params, b2):\n",
    "#     params['W2'] = W2\n",
    "#     params['b2'] = b2\n",
    "#     params['W1'] = W1\n",
    "    params['b2'] = b2\n",
    "\n",
    "    W2 = params['W2']\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    h, y, cost = forward_prop(data, labels, params)\n",
    "    loss = cost * data.shape[0]\n",
    "    \n",
    "    delta_1 = y - labels\n",
    "    gradW2 = np.dot(h.T, delta_1)   # matrix multiple sums the grad over all samples in the batch\n",
    "    gradb2 = np.sum(delta_1, axis=0, keepdims=True) # need to sum up mannually\n",
    "    \n",
    "    delta_2 = np.multiply(np.dot(delta_1, W2.T), h * (1 - h))\n",
    "    gradW1 = np.dot(data.T, delta_2)\n",
    "#     gradb1 = np.sum(delta_2, axis=0, keepdims=True)\n",
    "\n",
    "    return loss, gradb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using\n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    N = 10\n",
    "    D = 10\n",
    "    H = 5\n",
    "    K = 10\n",
    "    dimensions = [D, H, K]\n",
    "    data = np.random.randn(N, D)   # each row will be a datum\n",
    "    labels = np.zeros((N, K))\n",
    "    for i in range(N):\n",
    "        labels[i, random.randint(0, K - 1)] = 1\n",
    "    \n",
    "    params = {}\n",
    "    W1 = np.random.standard_normal((D, H))\n",
    "    b1 = np.zeros((1, H), dtype=float)\n",
    "    W2 = np.random.standard_normal((H, K))\n",
    "    b2 = np.zeros((1, K), dtype=float)\n",
    "\n",
    "    params['W1'] = W1\n",
    "    params['b1'] = b1\n",
    "    params['W2'] = W2\n",
    "    params['b2'] = b2\n",
    "    \n",
    "    gradcheck_naive(\n",
    "        lambda W1: back_prop_grad_check(data, labels, params, b2), b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
